{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.layers.merge import Maximum, Concatenate\n",
    "# from keras.layers import Maximum, Concatenate\n",
    "from keras.models import Model #Keras의 Model 클래스를 가져온다, 이 클래스는 Keras 텐서를 인스턴스화하는 데 사용\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier #RandomForest Classifier를 가져옴.(앙상블 학습방법) \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as  tf\n",
    "#import tensorflow.compat.v1 as tf\n",
    "#tf.compat.v1.disable_eager_execution() # work with tensorflow 2.0\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from model import Model as Classifier #사용자가 정의한 모듈에서 model을 가져와 Classifier로 별칭을 지정\n",
    "import pickle #pickle 모듈은 python 객체 구조의 직렬화 및 역직렬화를 위한 이진 프로토콜을 구현\n",
    "from sklearn.datasets import load_svmlight_file #libsvm 형식의 데이터셋을 로드하는데 사용할 수 있는 함수를 가져온다.\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set(style = \"white\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test data is a list of 3435 malware examples, each with 3514 API features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_dict = pickle.load(open('feat_dict.pickle', 'rb'), encoding='latin1') #pickle 파일 로드,\n",
    "features = []\n",
    "sha1 = []\n",
    "for key in seed_dict:\n",
    "    seed_dict[key] = seed_dict[key].toarray()[0] \n",
    "    features.append(seed_dict[key])\n",
    "    sha1.append(key)\n",
    "feed_feat = np.stack(features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset contains 13190 samples, each with 3514 API features. The first 6896 are malware example, and the last 6294 are benign examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train MalGAN and plot result\n",
    "\n",
    "# models = ['adv_keep_twocls', 'adv_del_twocls']\n",
    "#models = ['baseline_adv_delete_one', 'baseline_adv_insert_one', 'baseline_adv_delete_two', \\\n",
    "          #'baseline_adv_insert_rootallbutone', 'baseline_adv_combine_two']\n",
    "models = ['baseline_checkpoint', 'robust_delete_one', 'robust_insert_one', 'robust_delete_two', \\\n",
    "          'robust_insert_allbutone', 'robust_monotonic', 'robust_combine_two_v2_e18', 'robust_combine_three_e17']\n",
    "\n",
    "dataframes = []\n",
    "for model in models:\n",
    "    malgan = MalGAN(model)\n",
    "    df = malgan.train(epochs = 50, batch_size= 128)\n",
    "    dataframes.append(df)\n",
    "\n",
    "data = pd.concat(dataframes, axis = 0).reset_index(drop=True)\n",
    "plt.figure()\n",
    "\n",
    "g = sns.lineplot(x = 'L0', y = 'ERA', data=data, hue='model')\n",
    "plt.xlabel(\"$L_0$\")\n",
    "g.set(yticks = [0.00, 0.25, 0.50, 0.75, 1.00])\n",
    "g.xaxis.set_major_locator(ticker.FixedLocator([10, 200, 500, 1000, 2000, 3514]))\n",
    "handles, labels = g.get_legend_handles_labels()\n",
    "g.legend(handles=handles, labels=labels)\n",
    "fig = g.get_figure()\n",
    "fig.savefig(\"result.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MalGAN():\n",
    "    def __init__(self, model_name):\n",
    "        #API 피처의 차원 수와 노이즈 벡터의 차원 수를  설정\n",
    "        self.apifeature_dims = 3514\n",
    "        self.z_dims = 100   # noise appended at the end of example\n",
    "        self.model_name = model_name\n",
    "\n",
    "        self.hide_layers = 256\n",
    "        self.generator_layers = [self.apifeature_dims+self.z_dims, self.hide_layers, self.apifeature_dims]\n",
    "        self.substitute_detector_layers = [self.apifeature_dims, self.hide_layers, 1]\n",
    "        #사전에 학습된 모델을 로드하여 Black-box 판별자를 구성합니다.\n",
    "        self.blackbox, self.sess = self.build_blackbox_detector(self.model_name) \n",
    "        self.optimizer = Adam(lr=0.001)\n",
    "        \n",
    "        # Build and compile the substitute_detector\n",
    "        # 판별자 구성: 생성자가 생성한 악성코드 샘플을 입력으로 받아 이 샘플이 진짜인지 가짜인지를 판별하는 역할\n",
    "        self.substitute_detector = self.build_substitute_detector()\n",
    "        self.substitute_detector.compile(loss='binary_crossentropy', optimizer=self.optimizer, metrics=['accuracy'])\n",
    "        \n",
    "        \n",
    "        # Build the generator\n",
    "        #생성자는 실제 데이터와 노이즈를 입력으로 받아 새로운 데이터를 생성하는 역할\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes malware and noise as input and generates adversarial malware examples\n",
    "        #생성자에 입력으로 들어갈 실제 악성 코드 피처와 노이즈에 대한 입력 레이어를 정의, 이 두 입력 레이어를 input이라는 리스트에 저장\n",
    "        example = Input(shape=(self.apifeature_dims,))\n",
    "        noise = Input(shape=(self.z_dims,))\n",
    "        input = [example, noise]\n",
    "        malware_examples = self.generator(input)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        #GAN 훈련과정에서 생성자만 훈련하기 위해 판별자의 가중치를 고정\n",
    "        self.substitute_detector.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        #판별자는 생성자가 생성한 새로운 악성코드 샘플을 입력으로 받아 그 샘플이 진짜 악성코드인지 아닌지 판별\n",
    "        validity = self.substitute_detector(malware_examples)\n",
    "\n",
    "        # The combined model  (stacked generator and substitute_detector)\n",
    "        self.combined = Model(input, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=self.optimizer)\n",
    "        \n",
    "        \n",
    "    def build_generator(self):\n",
    "        #생성자는 실제 데이터와 노이즈를 입력으로 받아 새로운 데이터를 생성하는 역할\n",
    "        #각각의 코드는 생성자에 입력될 실제 악성코드 피처와 노이즈에 대한 입력 레이어를 정의\n",
    "        example = Input(shape=(self.apifeature_dims,))\n",
    "        noise = Input(shape=(self.z_dims,))\n",
    "        x = Concatenate(axis=1)([example, noise]) # example과 noise를 연결하여 하나의 벡터로 만든다. 이렇게 만들어진 벡터가 생성자의 입력으로 사용\n",
    "        for dim in self.generator_layers[1:]:\n",
    "            x = Dense(dim)(x)\n",
    "            x = Activation(activation='sigmoid')(x)\n",
    "        x = Maximum()([example, x])\n",
    "        #입력 레이어(example and noise) 출력 레이어(x)를 이용해 케라스 모델을 생성\n",
    "        generator = Model([example, noise], x, name='generator')\n",
    "        generator.summary()\n",
    "        return generator\n",
    "     # 판별자 구성: 생성자가 생성한 악성코드 샘플을 입력으로 받아 이 샘플이 진짜인지 가짜인지를 판별하는 역할\n",
    "    def build_substitute_detector(self):\n",
    "\n",
    "        input = Input(shape=(self.substitute_detector_layers[0],))\n",
    "        x = input #입력레이어를 x에 저장\n",
    "        for dim in self.substitute_detector_layers[1:]:\n",
    "            x = Dense(dim)(x)\n",
    "            x = Activation(activation='sigmoid')(x)\n",
    "        substitute_detector = Model(input, x, name='substitute_detector') # 입력 레이어와 출력레이어를 이용해 케라스 모델을 생성 이 모델이 바로 판별자\n",
    "        substitute_detector.summary()\n",
    "        return substitute_detector    \n",
    "    \n",
    "\n",
    "    def build_blackbox_detector(self,model_name):\n",
    "        #저장된 모델의 경로를 설정 .ckpt확장자 \n",
    "        PATH = \"adv_trained/{}.ckpt\".format(model_name)\n",
    "        # Clear the current graph in each run, to avoid variable duplication\n",
    "        tf.reset_default_graph()\n",
    "        model = Classifier() #외부에서 임포트 된 클래스\n",
    "        saver = tf.train.Saver() #모델의 변수를 저장하거나 불러오는 데 사용\n",
    "        sess = tf.Session() #텐서플로우의 연산을  실행\n",
    "        sess.run(tf.global_variables_initializer()) #전역 변수 지역 변수 초기화\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "                    \n",
    "        saver.restore(sess, PATH)#특정 세션에서 저장된 모델 변수를 불러옴\n",
    "        print (\"load model from:\", PATH) # 불러온 모델 경로 출력\n",
    "        \n",
    "        return model, sess\n",
    "        \n",
    "    def train(self, epochs, batch_size):\n",
    "        \n",
    "        model = self.blackbox\n",
    "        sess = self.sess\n",
    "        \n",
    "                \n",
    "        # Load test dataset (all malware)\n",
    "        #테스트 데이터셋 feat_dict.pickle 파일에서 로드: 이 데이터셋에는 malware 예시들만 포함되어 있음\n",
    "        seed_dict = pickle.load(open('feat_dict.pickle', 'rb'), encoding='latin1')\n",
    "        features = [] #dense한 벡터로 변환 되어 리스트에 저장\n",
    "        sha1 = [] #각 예시의 hash 값이 리스트에 저장\n",
    "        dist_dict = {} # [key]: hash [value]: L0 distance\n",
    "        for key in seed_dict:\n",
    "            seed_dict[key] = seed_dict[key].toarray()[0]\n",
    "            features.append(seed_dict[key])\n",
    "            sha1.append(key)\n",
    "        feed_feat = np.stack(features) #모든 malware 예시들을 하나의 numpy array로 쌓아 놓음\n",
    "        xtest_mal, ytest_mal = feed_feat, np.ones(len(feed_feat))# xtest_mal 이 array를 복사, ytest_mal은 각 malware 예시에 대응하는 레이블을 담음\n",
    "        \n",
    "\n",
    "        # Load training dataset\n",
    "        #train 파일 로드: 이 데이터셋은 benign예시와 malware 예시 모두 포함\n",
    "        train_x, train_y = load_svmlight_file(\"train_data.libsvm\",\n",
    "                                       n_features=3514,\n",
    "                                       multilabel=False, \n",
    "                                       zero_based=False,\n",
    "                                       query_id=False)\n",
    "        \n",
    "        train_x = train_x.toarray()\n",
    "        xtrain_ben = train_x[6896:] #benign 예시\n",
    "        ytrain_ben = train_y[6896:] #benign 레이블\n",
    "        xtrain_mal = train_x[0:6896] #malware 예시              \n",
    "        ytrain_mal = train_y[0:6896] #malware 레이블\n",
    "        \n",
    "        # Since the training dataset is unbalanced, we randomly choose sample from benign dataset\n",
    "        # and add them to the end to make up the gap\n",
    "        #학습 데이터셋은 benign 예시들이 malware예시들보다 적다. 따라서 benign 예시들 중 일부를 임의로 선택하여 데이터셋에 추가. 균형 맞추기\n",
    "        idx = np.random.randint(0, xtrain_ben.shape[0], 6896 - 6294) #benign 데이터셋에서 랜덤하게 선택할 인덱스를 생성. 선택할 개수는 현재의 malware 개수와 benign 개수 차이 만큼\n",
    "        add_on = xtrain_ben[idx] #생성한 인덱스를 사용하여 benign 데이터에서 해당 인덱스에 해당하는 예시를 선택\n",
    "        add_on_label = ytrain_ben[idx] # 선택된 bengin예시 레이블 선택\n",
    "        xtrain_ben = np.concatenate((xtrain_ben, add_on), axis=0) #기존 benign 데이터셋에 선택된 benign 예시를 추가\n",
    "        ytrain_ben = np.concatenate((ytrain_ben, add_on_label), axis = 0) #기존의 benign 레이블에 benign 레이블 추가\n",
    "\n",
    "        Test_TPR = []\n",
    "        d_loss_list, g_loss_list = [], []\n",
    "        \n",
    "        #각 epoch에 대한 substitute_detector와 generator의 훈련을 수행\n",
    "        #루프는 주어진 에폭 수만큼 모델 훈련을 반복한다.\n",
    "        for epoch in range(epochs): \n",
    "            \n",
    "            # Each epoch goes through all the data in the training set\n",
    "            start = 0                \n",
    "                \n",
    "            for step in range(xtrain_mal.shape[0] // batch_size): \n",
    "                \n",
    "                # ---------------------\n",
    "                #  Train substitute_detector\n",
    "                # ---------------------\n",
    "                #xmal_batch와xben_batch는 각각 악성 및 정상 데이터의 배치를 가져온다. noise는 생성자에 입력으로 사용되는 랜덤 노이즈를 생성\n",
    "                xmal_batch = xtrain_mal[start : start + batch_size]  \n",
    "                noise = np.random.uniform(0, 1, (batch_size, self.z_dims))\n",
    "\n",
    "                xben_batch = xtrain_ben[start : start + batch_size]\n",
    "                start = start + batch_size\n",
    "                \n",
    "                # predict using blackbox detector        \n",
    "                #blackbox 모델을 사용해 benign 배치에 대한 예측을 수행      \n",
    "                yben_batch = sess.run(model.y_pred,\\\n",
    "                    feed_dict={model.x_input:xben_batch})\n",
    "\n",
    "                # Generate a batch of new malware examples\n",
    "                #생성자는 gen_examples를 생성하는데 사용되며, 악성데이터 배치와 노이즈를 입력으로 사용\n",
    "                gen_examples = self.generator.predict([xmal_batch, noise])\n",
    "                #생성된 악성 예제에 대한 라벨을 생성: 라벨은 blackbox모델을 사용해 생성\n",
    "                ymal_batch = sess.run(model.y_pred,\\\n",
    "                                      feed_dict={model.x_input:np.ones(gen_examples.shape)*(gen_examples > 0.5)})\n",
    "                \n",
    "                # Train the substitute_detector\n",
    "                #substitute_decoder는 실제 benign 데이터와 생성된 악성 예쩨에 대해 훈련. 손실 계산\n",
    "                d_loss_real = self.substitute_detector.train_on_batch(xben_batch, yben_batch)\n",
    "                d_loss_fake = self.substitute_detector.train_on_batch(gen_examples, ymal_batch)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "                \n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "                #생성자는 악성 데이터 배치와 노이즈를 입력으로 사용해 훈련\n",
    "                #이는 생성자가 substitute_detector를 속이도록 만드는데 악성예제를 0으로 분류\n",
    "                noise = np.random.uniform(0, 1, (batch_size, self.z_dims))\n",
    "                g_loss = self.combined.train_on_batch([xmal_batch,noise], np.zeros((batch_size, 1)))\n",
    "\n",
    "            \n",
    "            # After each epoch, Evaluate evasion performance on the test dataset\n",
    "            # try different noise for 3 times\n",
    "            for j in range(3): # 새로운 노이즈를 사용하여 악성예제를 생성하고, 그 성능을 평가하는 과정을 세 번 반복\n",
    "                noise = np.random.uniform(0, 1, (xtest_mal.shape[0], self.z_dims))\n",
    "                gen_examples = self.generator.predict([xtest_mal, noise]) #테스트 세트의 악성 데이터와 생성된 노이즈를 사용하여 생성자로 부터 얻은 악성예제\n",
    "                #얼마나 잘 분류하는지를 측정    \n",
    "                TPR = sess.run(model.accuracy,\\\n",
    "                        feed_dict={model.x_input:np.ones(gen_examples.shape)*(gen_examples > 0.5), model.y_input: np.ones(gen_examples.shape[0],)})\n",
    "            \n",
    "                Test_TPR.append(TPR) \n",
    "                #생성된 예제를 이진 형태로 변환. 생성된 예제의 값이 0보다 크면 1 \n",
    "                transformed_to_bin = np.ones(gen_examples.shape)*(gen_examples > 0.5)\n",
    "                #blackbox model을 사용하여 변환된 악성 예제에 대한 예측 라벨을 생성\n",
    "                pred_y_label = sess.run(model.y_pred,\\\n",
    "                                     feed_dict={model.x_input:np.ones(gen_examples.shape)*(gen_examples > 0.5)})\n",
    "            \n",
    "            \n",
    "            \n",
    "                # remove successfully evaded malware examples from xtest_mal\n",
    "                #생성된 악성 예제가 blackbox 피하는데 성공한 경우, 실제로는 악성이지만 blackbox 모델이 정상으로 분류한 경우 테스트 세트에서 제거\n",
    "                #해당 예제와 원본 예제간의 LO 거리를 계산하여 저장\n",
    "                #LO거리는 두 벡터간의 다른 두 요소의 수를 나타내므로, 원본 악성 예제와 생성된 악성 예제간의 차이를 나타냄\n",
    "                i = 0\n",
    "                while i  < pred_y_label.shape[0]:\n",
    "                    if pred_y_label[i] == 0: # should be 1 but predict 0\n",
    "                    #print(sha1[i], xtrain_mal[i])\n",
    "                    # calculate L0 distance and put to dictionary\n",
    "                        L0 = np.sum(transformed_to_bin[i]) - np.sum(xtest_mal[i]) #insertion only\n",
    "                        dist_dict[sha1[i]] = L0  # [key]: hash [value]: L0 distance\n",
    "                        xtest_mal = np.delete(xtest_mal, i, 0)\n",
    "                        pred_y_label = np.delete(pred_y_label, i, 0)\n",
    "                        sha1 = sha1[:i] + sha1[i+1:]\n",
    "                    else:\n",
    "                        i += 1\n",
    "                # 현재 테스트 세트에 남아있는 악성 예제의 수를 출력합니다. \n",
    "                print(\"remaining malware examples:\", xtest_mal.shape[0])\n",
    "                if xtest_mal.shape[0] == 0:\n",
    "                    break #successful evade all\n",
    "\n",
    "        \n",
    "            # Print and record the progress\n",
    "            print(\"[MalGAN] epoch(%d) [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch+1, d_loss[0], 100*d_loss[1], g_loss))\n",
    "            print(\"[Classifier] Test TPR on remaining test data: %f\" % (Test_TPR[-1]))\n",
    "            d_loss_list.append(d_loss[0])\n",
    "            g_loss_list.append(g_loss)\n",
    "            if xtest_mal.shape[0] == 0:\n",
    "                break #successful evade all\n",
    "            \n",
    "\n",
    "        sess.close()\n",
    "        \n",
    "        # AFTER all epochs\n",
    "        # Plot the progress --> loss\n",
    "        # d_loss_df = pd.DataFrame(dict(epoch =np.arange(1, len(d_loss_list)+1),\\\n",
    "        #                               dataset = \"d loss\",\\\n",
    "        #                               loss = np.asarray(d_loss_list, dtype=np.float32)))\n",
    "        # g_loss_df = pd.DataFrame(dict(epoch =np.arange(1, len(g_loss_list)+1),\\\n",
    "        #                               dataset = \"g loss\",\\\n",
    "        #                               loss = np.asarray(g_loss_list, dtype=np.float32)))\n",
    "\n",
    "        # loss_df = pd.concat([d_loss_df,g_loss_df], axis=0).reset_index(drop=True)      \n",
    "        # plt.figure()\n",
    "        # loss_plot = sns.lineplot(x = 'epoch', y = 'loss', hue ='dataset', data = loss_df)\n",
    "        # handles, labels = loss_plot.get_legend_handles_labels()\n",
    "        # loss_plot.legend(handles=handles, labels=labels)\n",
    "        # plt.show()\n",
    "        # fig = loss_plot.get_figure()\n",
    "        # fig.savefig(\"{}_loss\".format(self.model_name))  # save loss plot\n",
    "        \n",
    "        # get corresponding dataframe for different models\n",
    "        #ERA는 성공적으로 피해진 악성 예제의 비율을 나타내는 지표\n",
    "        ERA = []\n",
    "        success_num = 0\n",
    "        # Calculate ERA for each L0 distance\n",
    "        #LO거리가 i인 각 악성예제에 대해, 해당 예제가 Black Box 모델을 피하는데 성공했다면 success_num을 증가\n",
    "        #LO거리는 원래의 악성예제와 생성된 악성 예제 간에 변화된 피처의 수\n",
    "        #남아있는 악성예제의 수를 전체 악성 예제 수로 나눈다.\n",
    "        for i in range(3515): # 0 - 3514 features\n",
    "            for key in dist_dict:\n",
    "                if dist_dict[key] == i:\n",
    "                    success_num += 1\n",
    "            ERA.append((3435 - success_num) / 3435)\n",
    "        \n",
    "        # report ERA if not completely evaded\n",
    "        #만약 모든 악성예제가 피하지 못하였다면 이를 출력하여 리포팅\n",
    "        if ERA[-1] != 0:\n",
    "            print(\"{} is not completely evaded after {} epochs. ERA = {}\".format(self.model_name, epochs, ERA[-1]))\n",
    "            \n",
    "        #이 부분은 모델의 이름에 따라 그래프의 이름을 지정    \n",
    "        if self.model_name == 'baseline_checkpoint': curve_name = 'Baseline'\n",
    "        if self.model_name == \"baseline_adv_delete_one\": curve_name = 'Adv Retrain A'\n",
    "        if self.model_name == \"robust_delete_one\": curve_name = 'Robust A' \n",
    "        if self.model_name == \"baseline_adv_insert_one\": curve_name = 'Adv Retrain B'\n",
    "        if self.model_name == \"robust_insert_one\": curve_name = 'Robust B'\n",
    "        if self.model_name == \"baseline_adv_delete_two\": curve_name = 'Adv Retrain C'\n",
    "        if self.model_name == \"robust_delete_two\": curve_name = 'Robust C'\n",
    "        if self.model_name == \"baseline_adv_insert_rootallbutone\": curve_name = 'Adv Retrain D'\n",
    "        if self.model_name == \"adv_keep_twocls\": curve_name = 'Ensemble D Base Learner'\n",
    "        if self.model_name == \"robust_monotonic\": curve_name = 'Robust E'\n",
    "        if self.model_name == \"baseline_adv_combine_two\": curve_name = 'Adv Retrain A+B'\n",
    "        if self.model_name == \"adv_del_twocls\": curve_name = 'Ensemble A+B Base Learner'\n",
    "        if self.model_name == \"robust_combine_two_v2_e18\": curve_name = 'Robust A+B'\n",
    "        if self.model_name == \"robust_insert_allbutone\": curve_name = 'Robust D'\n",
    "        if self.model_name == \"robust_combine_three_e17\": curve_name = 'Robust A+B+E'\n",
    "        #각 LO거리에 대한 ERA를 포함하는 데이터 프레임을 만들고 반환(ERA, 모델이름, 그리고 LO 거리를 각각 포함)\n",
    "        model_df = pd.DataFrame(dict(ERA = np.asarray(ERA, dtype=np.float32),\\\n",
    "                                     model = curve_name, L0 = np.arange(3515)))\n",
    "        return model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MalGAN and plot result\n",
    "\n",
    "# models = ['adv_keep_twocls', 'adv_del_twocls']\n",
    "#models = ['baseline_adv_delete_one', 'baseline_adv_insert_one', 'baseline_adv_delete_two', \\\n",
    "          #'baseline_adv_insert_rootallbutone', 'baseline_adv_combine_two']\n",
    "#여러 모델을 대상으로 MalGAN을 학습시키고 그 결과를 그래프로 표시하는 과정\n",
    "models = ['baseline_checkpoint', 'robust_delete_one', 'robust_insert_one', 'robust_delete_two', \\\n",
    "          'robust_insert_allbutone', 'robust_monotonic', 'robust_combine_two_v2_e18', 'robust_combine_three_e17']\n",
    "#각 모델에 대해 MalGAN을 생성하고 학습을 수행\n",
    "dataframes = []\n",
    "for model in models:\n",
    "    malgan = MalGAN(model)\n",
    "    df = malgan.train(epochs = 50, batch_size= 128)\n",
    "    dataframes.append(df)\n",
    "\n",
    "data = pd.concat(dataframes, axis = 0).reset_index(drop=True)\n",
    "plt.figure()\n",
    "\n",
    "g = sns.lineplot(x = 'L0', y = 'ERA', data=data, hue='model')\n",
    "plt.xlabel(\"$L_0$\")\n",
    "g.set(yticks = [0.00, 0.25, 0.50, 0.75, 1.00])\n",
    "g.xaxis.set_major_locator(ticker.FixedLocator([10, 200, 500, 1000, 2000, 3514]))\n",
    "handles, labels = g.get_legend_handles_labels()\n",
    "g.legend(handles=handles, labels=labels)\n",
    "fig = g.get_figure()\n",
    "fig.savefig(\"result.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
